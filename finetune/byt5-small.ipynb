{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.status.idle": "2025-06-10T13:32:55.495568Z",
     "shell.execute_reply": "2025-06-10T13:32:55.494889Z",
     "shell.execute_reply.started": "2025-06-10T13:32:53.531585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"jiacheng-ye/nl2bash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T13:32:57.121339Z",
     "iopub.status.busy": "2025-06-10T13:32:57.121148Z",
     "iopub.status.idle": "2025-06-10T13:32:57.125305Z",
     "shell.execute_reply": "2025-06-10T13:32:57.124573Z",
     "shell.execute_reply.started": "2025-06-10T13:32:57.121324Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['nl', 'bash'],\n",
      "        num_rows: 8090\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['nl', 'bash'],\n",
      "        num_rows: 609\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['nl', 'bash'],\n",
      "        num_rows: 606\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T13:32:57.696982Z",
     "iopub.status.busy": "2025-06-10T13:32:57.696243Z",
     "iopub.status.idle": "2025-06-10T13:32:57.701501Z",
     "shell.execute_reply": "2025-06-10T13:32:57.700649Z",
     "shell.execute_reply.started": "2025-06-10T13:32:57.696956Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples :  8090\n",
      "Testing Samples :  606\n",
      "Validation Samples :  609\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "validation_dataset = dataset['validation']\n",
    "\n",
    "print(\"Training Samples : \",len(train_dataset))\n",
    "print(\"Testing Samples : \",len(test_dataset))\n",
    "print(\"Validation Samples : \",len(validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T13:33:01.956762Z",
     "iopub.status.busy": "2025-06-10T13:33:01.956459Z",
     "iopub.status.idle": "2025-06-10T13:33:01.960403Z",
     "shell.execute_reply": "2025-06-10T13:33:01.959574Z",
     "shell.execute_reply.started": "2025-06-10T13:33:01.956743Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"google/byt5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T13:33:04.897155Z",
     "iopub.status.busy": "2025-06-10T13:33:04.896431Z",
     "iopub.status.idle": "2025-06-10T13:33:09.071100Z",
     "shell.execute_reply": "2025-06-10T13:33:09.070563Z",
     "shell.execute_reply.started": "2025-06-10T13:33:04.897130Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# tokenizer \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T13:33:10.438159Z",
     "iopub.status.busy": "2025-06-10T13:33:10.437108Z",
     "iopub.status.idle": "2025-06-10T13:33:10.444932Z",
     "shell.execute_reply": "2025-06-10T13:33:10.443903Z",
     "shell.execute_reply.started": "2025-06-10T13:33:10.438122Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "    inputs = tokenizer(\n",
    "        [\"Translate to Bash: \" + nl for nl in example[\"nl\"]],\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    targets = tokenizer(\n",
    "        [bash for bash in example[\"bash\"]], \n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    labels = targets[\"input_ids\"]\n",
    "    labels = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in label_seq]\n",
    "        for label_seq in labels\n",
    "    ]\n",
    "    \n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T13:33:14.302034Z",
     "iopub.status.busy": "2025-06-10T13:33:14.301472Z",
     "iopub.status.idle": "2025-06-10T13:33:21.227813Z",
     "shell.execute_reply": "2025-06-10T13:33:21.227181Z",
     "shell.execute_reply.started": "2025-06-10T13:33:14.302009Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9379ca03da04864badbcb98a5caf0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8090 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4746512aa65b4a1db3d2b3e4eec3be0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450cd0b4545544e7b37a6ee614fa60dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/609 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nl': \"Do a dry run of renaming file extension '.andnav' to '.tile' for all files/directories under current directory tree\", 'bash': 'find . -name \"*.andnav\" | rename -vn \"s/\\\\.andnav$/.tile/\"', 'input_ids': [87, 117, 100, 113, 118, 111, 100, 119, 104, 35, 119, 114, 35, 69, 100, 118, 107, 61, 35, 71, 114, 35, 100, 35, 103, 117, 124, 35, 117, 120, 113, 35, 114, 105, 35, 117, 104, 113, 100, 112, 108, 113, 106, 35, 105, 108, 111, 104, 35, 104, 123, 119, 104, 113, 118, 108, 114, 113, 35, 42, 49, 100, 113, 103, 113, 100, 121, 42, 35, 119, 114, 35, 42, 49, 119, 108, 111, 104, 42, 35, 105, 114, 117, 35, 100, 111, 111, 35, 105, 108, 111, 104, 118, 50, 103, 108, 117, 104, 102, 119, 114, 117, 108, 104, 118, 35, 120, 113, 103, 104, 117, 35, 102, 120, 117, 117, 104, 113, 119, 35, 103, 108, 117, 104, 102, 119, 114, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [105, 108, 113, 103, 35, 49, 35, 48, 113, 100, 112, 104, 35, 37, 45, 49, 100, 113, 103, 113, 100, 121, 37, 35, 127, 35, 117, 104, 113, 100, 112, 104, 35, 48, 121, 113, 35, 37, 118, 50, 95, 49, 100, 113, 103, 113, 100, 121, 39, 50, 49, 119, 108, 111, 104, 50, 37, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenization,batched=True)\n",
    "test_dataset = test_dataset.map(tokenization,batched=True)\n",
    "validation_dataset = validation_dataset.map(tokenization,batched=True)\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T13:33:50.292452Z",
     "iopub.status.busy": "2025-06-10T13:33:50.291835Z",
     "iopub.status.idle": "2025-06-10T13:33:55.671099Z",
     "shell.execute_reply": "2025-06-10T13:33:55.670508Z",
     "shell.execute_reply.started": "2025-06-10T13:33:50.292428Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 13:33:50.738862: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749562430.760918     232 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749562430.767674     232 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# model \n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T13:33:58.657191Z",
     "iopub.status.busy": "2025-06-10T13:33:58.656355Z",
     "iopub.status.idle": "2025-06-10T13:33:58.660811Z",
     "shell.execute_reply": "2025-06-10T13:33:58.660003Z",
     "shell.execute_reply.started": "2025-06-10T13:33:58.657164Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hf_token = \"your_hugging_face_token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T13:34:01.112406Z",
     "iopub.status.busy": "2025-06-10T13:34:01.111775Z",
     "iopub.status.idle": "2025-06-10T13:34:01.158359Z",
     "shell.execute_reply": "2025-06-10T13:34:01.157844Z",
     "shell.execute_reply.started": "2025-06-10T13:34:01.112380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## login to hf \n",
    "from huggingface_hub import login\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T13:34:03.356853Z",
     "iopub.status.busy": "2025-06-10T13:34:03.356544Z",
     "iopub.status.idle": "2025-06-10T13:34:04.050695Z",
     "shell.execute_reply": "2025-06-10T13:34:04.050029Z",
     "shell.execute_reply.started": "2025-06-10T13:34:03.356831Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# evaluation metric \n",
    "from evaluate import load \n",
    "bleu_metric = load(\"bleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    print(eval_preds)\n",
    "    preds, labels = eval_preds\n",
    "    # print(\"------- preds -----------\",preds)\n",
    "    # print(\"-------labels------------\",labels)\n",
    "\n",
    "    #replacing -100 with pad_token_id \n",
    "    preds = [\n",
    "        [(token if token != -100 else tokenizer.pad_token_id) for token in preds_seq]\n",
    "        for preds_seq in preds\n",
    "    ]\n",
    "    labels = [\n",
    "        [(token if token!= -100 else tokenizer.pad_token_id) for token in labels_seq]\n",
    "        for labels_seq in labels \n",
    "    ]\n",
    "    # print(\"------- preds -----------\",preds)\n",
    "    # print(\"-------labels------------\",labels)\n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # BLEU expects list of references (list of list of strings) and list of predictions\n",
    "    decoded_labels = [[label] for label in decoded_labels]\n",
    "\n",
    "    result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    print(result)\n",
    "    return {\"bleu\": result[\"bleu\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T13:34:07.472635Z",
     "iopub.status.busy": "2025-06-10T13:34:07.472062Z",
     "iopub.status.idle": "2025-06-10T13:34:07.512379Z",
     "shell.execute_reply": "2025-06-10T13:34:07.511841Z",
     "shell.execute_reply.started": "2025-06-10T13:34:07.472611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/kaggle/working/byt5-small-fine-tuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=64,\n",
    "    disable_tqdm=False,       \n",
    "    report_to=\"none\", \n",
    "    hub_model_id=\"archan01/byt5-small-finetuned\",\n",
    "    hub_strategy=\"every_save\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T13:34:11.075911Z",
     "iopub.status.busy": "2025-06-10T13:34:11.074867Z",
     "iopub.status.idle": "2025-06-10T13:34:11.803572Z",
     "shell.execute_reply": "2025-06-10T13:34:11.802923Z",
     "shell.execute_reply.started": "2025-06-10T13:34:11.075883Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_232/3683660493.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer,default_data_collator\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T13:34:14.300276Z",
     "iopub.status.busy": "2025-06-10T13:34:14.299649Z",
     "iopub.status.idle": "2025-06-10T15:49:19.444751Z",
     "shell.execute_reply": "2025-06-10T15:49:19.441859Z",
     "shell.execute_reply.started": "2025-06-10T13:34:14.300250Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10120' max='10120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10120/10120 2:15:02, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.074100</td>\n",
       "      <td>0.745262</td>\n",
       "      <td>0.244871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.690900</td>\n",
       "      <td>0.644243</td>\n",
       "      <td>0.284142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.590300</td>\n",
       "      <td>0.587402</td>\n",
       "      <td>0.308416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.527200</td>\n",
       "      <td>0.552282</td>\n",
       "      <td>0.321590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.488400</td>\n",
       "      <td>0.532060</td>\n",
       "      <td>0.330956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.454700</td>\n",
       "      <td>0.520522</td>\n",
       "      <td>0.347698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.432700</td>\n",
       "      <td>0.512733</td>\n",
       "      <td>0.349912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.413400</td>\n",
       "      <td>0.510413</td>\n",
       "      <td>0.351045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.397500</td>\n",
       "      <td>0.507435</td>\n",
       "      <td>0.358093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.388900</td>\n",
       "      <td>0.507995</td>\n",
       "      <td>0.357527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7ebfe97092d0>\n",
      "{'bleu': 0.24487121212333413, 'precisions': [0.47383000437381545, 0.3128, 0.22155264090747961, 0.17225640007938084], 'brevity_penalty': 0.8928989883017904, 'length_ratio': 0.8982451545311682, 'translation_length': 6859, 'reference_length': 7636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7ebfea5d9e50>\n",
      "{'bleu': 0.2841416554230989, 'precisions': [0.5579325421611493, 0.3908541846419327, 0.28527370855821127, 0.22619307038570494], 'brevity_penalty': 0.8249935072748157, 'length_ratio': 0.8386589837611315, 'translation_length': 6404, 'reference_length': 7636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7ebfe94d1550>\n",
      "{'bleu': 0.308416282974707, 'precisions': [0.5798085828959555, 0.41352871017209064, 0.3083064056263068, 0.2502144082332762], 'brevity_penalty': 0.8363075287395868, 'length_ratio': 0.8483499214248298, 'translation_length': 6478, 'reference_length': 7636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7ec15c809e50>\n",
      "{'bleu': 0.321589598838855, 'precisions': [0.5936231012182283, 0.42185430463576157, 0.3116142094607031, 0.24819326863514352], 'brevity_penalty': 0.8620488193515043, 'length_ratio': 0.8707438449449973, 'translation_length': 6649, 'reference_length': 7636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7ebfe030f710>\n",
      "{'bleu': 0.330956103513284, 'precisions': [0.5980276714748307, 0.42021018593371057, 0.3116929698708752, 0.25145494681918523], 'brevity_penalty': 0.8834391336007419, 'length_ratio': 0.889732844421163, 'translation_length': 6794, 'reference_length': 7636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7ebfe04a62d0>\n",
      "{'bleu': 0.34769846661838993, 'precisions': [0.6086377754770005, 0.4413198959687906, 0.33489630297565376, 0.2726538849646821], 'brevity_penalty': 0.878605994180897, 'length_ratio': 0.8854112100576218, 'translation_length': 6761, 'reference_length': 7636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7ebfe8138f50>\n",
      "{'bleu': 0.3499115843316061, 'precisions': [0.6405472636815921, 0.4688304997424008, 0.3572249904177846, 0.29546436285097194], 'brevity_penalty': 0.82928694094871, 'length_ratio': 0.8423258250392875, 'translation_length': 6432, 'reference_length': 7636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7ebfe953cbd0>\n",
      "{'bleu': 0.3510446258968792, 'precisions': [0.6388888888888888, 0.4650414762146606, 0.3513870541611625, 0.2892948173322005], 'brevity_penalty': 0.8420763500985253, 'length_ratio': 0.8533263488737559, 'translation_length': 6516, 'reference_length': 7636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7ebfe04e7410>\n",
      "{'bleu': 0.35809304975273043, 'precisions': [0.6447870554113876, 0.4681925277684281, 0.3567679040119985, 0.2961246840775063], 'brevity_penalty': 0.8473651487383967, 'length_ratio': 0.857909900471451, 'translation_length': 6551, 'reference_length': 7636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7ebfe021aa50>\n",
      "{'bleu': 0.3575274945055306, 'precisions': [0.6376964933494559, 0.46295988013983685, 0.35198221563542054, 0.2913287585776669], 'brevity_penalty': 0.8571248357919484, 'length_ratio': 0.8664222105814563, 'translation_length': 6616, 'reference_length': 7636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10120, training_loss=0.5457962609091295, metrics={'train_runtime': 8104.7915, 'train_samples_per_second': 9.982, 'train_steps_per_second': 1.249, 'total_flos': 1.85817101991936e+16, 'train_loss': 0.5457962609091295, 'epoch': 10.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T15:49:46.706884Z",
     "iopub.status.busy": "2025-06-10T15:49:46.706358Z",
     "iopub.status.idle": "2025-06-10T15:50:17.775221Z",
     "shell.execute_reply": "2025-06-10T15:50:17.774465Z",
     "shell.execute_reply.started": "2025-06-10T15:49:46.706862Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066689ee853d46b28fb401876b3cd556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d822726b1c1d4fd9a6f79aa70d4b0358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb26f40d7b04bca8d0f8d63d0befb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/archan01/byt5-small-finetuned/commit/08e6a7aac70d1f1519dab91eafcdc6af281b96d8', commit_message='End of training', commit_description='', oid='08e6a7aac70d1f1519dab91eafcdc6af281b96d8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/archan01/byt5-small-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='archan01/byt5-small-finetuned'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
